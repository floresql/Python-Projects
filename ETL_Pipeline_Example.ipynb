{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ec9379-11f1-4f40-8e42-41fa3ec477ff",
   "metadata": {},
   "source": [
    "# ETL Pipeline with Python: Extract, Transform, and Load\n",
    "\n",
    "This script demonstrates how to build an ETL (Extract, Transform, Load) pipeline using Python. The pipeline uses libraries such as `pandas`, `requests`, and `sqlite3`, and integrates with SQL databases via `SQLAlchemy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "454d0b09-45bd-4cab-94bf-16b21543b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f5d0a-4a2e-4934-b184-09aae48e731c",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "## 1. **Extract Data**\n",
    "   - **From CSV**: Load data from a CSV file using `pandas.read_csv`.\n",
    "   - **From API**: Fetch and normalize JSON data from an API using `requests` and `pandas.json_normalize`.\n",
    "   - **From SQL**: Query data from an SQLite database using `pandas.read_sql_query`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36ba057d-9f7b-49a0-9a3c-1bdb2bb65daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_api(api_url):\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    return pd.json_normalize(response.json())  # Normalize JSON into a flat DataFrame\n",
    "\n",
    "def extract_sql(db_connection, query):\n",
    "    return pd.read_sql_query(query, db_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dd29e-2fc6-4f8a-9f03-dac025720654",
   "metadata": {},
   "source": [
    "## 2. **Transform Data**\n",
    "- Clean the data:\n",
    "    - Fill missing values with defaults or forward fill.\n",
    "- Rename columns for consistency.\n",
    "- Merge multiple datasets using `pd.concat`.\n",
    "- Add calculated columns by performing operations on existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f489cc-aefd-4d23-a072-da6024625d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df1, df2, df3):\n",
    "    df1.fillna({'column_name': 'default_value'}, inplace=True)\n",
    "    df2.fillna(0, inplace=True)\n",
    "    df3.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    df1.rename(columns={'old_name': 'new_name'}, inplace=True)\n",
    "    \n",
    "    combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "    combined_df['calculated_column'] = combined_df['column1'] + combined_df['column2']\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09518884-9c5e-4010-83dc-a084c3f5654a",
   "metadata": {},
   "source": [
    "## 3. **Load Data**\n",
    "- Save the transformed data to a SQL database table using `pandas.to_sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2e5a05-9f36-4116-84de-90db24990956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_sql(df, db_connection, table_name):\n",
    "    df.to_sql(table_name, db_connection, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee42279-5ded-47fb-a63b-309fbaa3ceb6",
   "metadata": {},
   "source": [
    "## 4. **ETL Pipeline Execution**\n",
    "- Set up paths, URLs, and database connections.\n",
    "- Extract data from various sources.\n",
    "- Transform the data to clean, merge, and prepare it.\n",
    "- Load the final dataset into an SQLite database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88dca657-1652-4cc0-b9a3-a96769f9034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during the ETL process: [Errno 2] No such file or directory: 'data.csv'\n"
     ]
    }
   ],
   "source": [
    "def etl_pipeline():\n",
    "    csv_path = 'data.csv'\n",
    "    api_url = 'https://api.example.com/data'\n",
    "    db_path = 'etl_pipeline.db'\n",
    "    sql_query = 'SELECT * FROM source_table'\n",
    "    \n",
    "    sqlite_conn = sqlite3.connect(db_path)\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "    \n",
    "    try:\n",
    "        csv_data = extract_csv(csv_path)\n",
    "        api_data = extract_api(api_url)\n",
    "        sql_data = extract_sql(sqlite_conn, sql_query)\n",
    "        \n",
    "        transformed_data = transform_data(csv_data, api_data, sql_data)\n",
    "        \n",
    "        load_data_to_sql(transformed_data, engine, 'etl_results')\n",
    "        \n",
    "        print(\"ETL pipeline executed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the ETL process: {e}\")\n",
    "    finally:\n",
    "        sqlite_conn.close()\n",
    "\n",
    "etl_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2656a8-605a-481e-9dbe-ee362e8eb700",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This ETL pipeline script demonstrates how to build a data integration workflow in Python. **Real data was not used in this example.** Instead, placeholder file paths, API URLs, and database queries were provided as examples to illustrate the process. For actual implementation, you'll need to replace these placeholders with real-world data sources.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The ETL pipeline includes the following steps:\n",
    "\n",
    "1. **Extract**:\n",
    "   - Data is loaded from various sources:\n",
    "     - CSV files using `pandas.read_csv`.\n",
    "     - APIs using `requests.get` and JSON normalization.\n",
    "     - SQL databases using `pandas.read_sql_query`.\n",
    "   - This provides flexibility in integrating data from diverse locations.\n",
    "\n",
    "2. **Transform**:\n",
    "   - Data cleaning (e.g., filling missing values, renaming columns).\n",
    "   - Dataset merging using `pandas.concat`.\n",
    "   - Adding calculated columns to enrich the data.\n",
    "\n",
    "3. **Load**:\n",
    "   - The transformed data is saved to an SQLite database table using `pandas.to_sql`.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Try/except blocks ensure the pipeline gracefully handles errors during execution.\n",
    "\n",
    "5. **Modularity**:\n",
    "   - Functions are modularized for reuse and scalability, making it easy to adapt the pipeline for other use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To adapt this pipeline to a real-world scenario:\n",
    "\n",
    "1. **Replace Placeholder Inputs**:\n",
    "   - Update the `csv_path`, `api_url`, and `sql_query` with real data sources.\n",
    "   - Example: Provide an actual file path, a working API endpoint, and a database connection.\n",
    "\n",
    "2. **Data Validation**:\n",
    "   - Ensure the data retrieved during extraction matches the expected structure and quality.\n",
    "   - Implement additional cleaning steps if needed.\n",
    "\n",
    "3. **Extend Transformations**:\n",
    "   - Add more complex transformation logic (e.g., data aggregation, filtering) as per your project requirements.\n",
    "\n",
    "4. **Database Configuration**:\n",
    "   - Test the pipeline with a larger database or use a cloud database instead of SQLite for scalability.\n",
    "\n",
    "5. **Performance Optimization**:\n",
    "   - Profile the script for runtime performance.\n",
    "   - Use chunking to handle large datasets or optimize database queries.\n",
    "\n",
    "6. **Automation**:\n",
    "   - Schedule the pipeline using tools like `cron`, `Apache Airflow`, or `Prefect`.\n",
    "\n",
    "By following these steps, you can implement and customize the ETL pipeline for your specific data integration and analysis needs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
